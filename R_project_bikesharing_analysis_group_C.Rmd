---
title: "bikesharing_analysis"
author: "Group C"
date: "12/16/2017"
output: html_document
---

Group C Members: Alexandre Olivier, Anchal Jaiswal, Asier sarasua Amundarain, Ignacio Rup√©rez Larrea, Jina Kim, Nicolas Clemens Linsenmaier, Rahul Singh



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Workplace setup

```{r library}
# import libraries
library(tidyr)
library(MASS)
library(ggplot2)
library(dplyr)
library(GGally)
library(gridExtra)
library(plyr)
library(car)
library(corrplot)
library(randomForest)
library(knitr)
library(effects)
library(caret)
library(HistData)
library(gvlma)
library(lmtest)
```


```{r import dataset}
# import dataset
bs <- read.csv("hour.csv")
```

```{r verify import}
# verify dataset has been loaded properly
head(bs,3)
```

# 1. Dataset overview
Source of dataset: http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
Original source: https://www.capitalbikeshare.com/system-data

This dataset contains the hourly and daily record of bike rental counts between year 2011 and 2012 in Washington D.C., provided by Capital Bikeshare, a bike rental company. This dataset also aggregates the weather and the seasonal information particular for that day, including temperature and humidity.
```{r dataset shape}
# dataset shape
dim(bs)
```

There are 17379 records with 17 columns in this dataset.
```{r dataset structure}
# look at structure of dataframe
str(bs)
```

The description of the 17 variables are as follows:

  * instant: record index
  * dteday: date of bike rental
  * season: season of bike rental
      + Note: the original data source indicates that spring is 1, but upon looking at the dates, it seems that 1 is actually winter, meaning the number code is 1: winter, 2: spring, 3: summer, 4: fall
  * yr: year of bike rental
  * mnth: month of bike rental
  * hr: hour of bike rental (0 to 23)
  * holiday: whether or not day of rental was a holiday
  * weekday: day of week of bike rental
  * workingday: whether or not day of rental was netiher a holiday, nor a weekend
  * weathersit: 
      + Clear, few clouds, partly cloudy
      + Mist, Mist + Cloudy, Mist + broken clouds, Mist + few clouds
      + Light snow, light rain + thundertsorm + scattered clouds, light rain + scattered clouds
      + Heavy rain + ice pallets + thunderstorm + mist, snow + fog
  * temp: normalized temperature in celsius (hourly scale)
      + normalization method: (t - t_min) / (t_max - t_min), 
      + t_min = -8, t_max = 39
  * atemp: normalized feeling temperature in celsius (hourly scale)
      + normalization method: (t - t_min) / (t_max - t_min)
      + t_min = -16, t_max = 50
  * hum: normalized humidity (values divided by 100, the max value)
  * windspeed: normalzed wind speed (values divided by 67, the max value)
  * casual: count of causal (non registered users)
  * registered: count of registered users of Capital Bikeshare
  * cnt: total count of rental bike (casual + registered)


Let's check if there are any missing values in this dataset
```{r missing values}
# check for missing values
# source: https://stackoverflow.com/questions/8317231/elegant-way-to-report-missing-values-in-a-data-frame
sapply(bs, function(x) sum(is.na(x)))
```
There are no missing values for any of the columns in this dataset.



# 2. Cleaning data

There are several columns that need to be cleaned or dropped:

  * instant: this is the index of the original data, which is not needed in R, because R has a default indexing applied to dataframes. This column will be dropped.
  * dteday: convert to datetypes using as.Date to perform date computations
  * season: change to original string value for clarity
  * weekday: change to original string value for clarity
  * temp: change back to original temperature value, as normalized values are hard to interpret
  * atemp: change back to original temperature value
  * hum: change back to original humidity
  * windspeed: change back to original windspeed
  * cnt: verify that casual + registered = cnt

```{r drop instant column}
# drop instant column
bs <- bs %>%
  dplyr::select(-instant)
```

```{r convert datetype column}
# convert dteday to date time data type
# source: https://www.statmethods.net/input/dates.html
bs$dteday <- as.Date(bs$dteday)

# verify column data type has changed
str(bs)
tail(bs, 3)
```

```{r convert string - season}
# change season back to original string value
bs$season = ifelse(bs$season == 1,"Winter",
                   ifelse(bs$season == 2,  "Spring",
                          ifelse(bs$season == 3, "Summer", "Fall")))

# verify changes
table(bs$season)
```

```{r convert string - dow}
# change weekday values back to original string value
bs$weekday = ifelse(bs$weekday == 1, "Mon",
                    ifelse(bs$weekday == 2, "Tues",
                           ifelse(bs$weekday == 3, "Wed",
                                  ifelse(bs$weekday ==4, "Thu",
                                         ifelse(bs$weekday == 5, "Fri",
                                                ifelse(bs$weekday == 6, "Sat", "Sun"))))))

# verify changes
table(bs$weekday)
```

```{r convert to original value - temp, atemp, hum, windspeed}
# change normalized values to original temp values
bs <- bs %>%
  mutate(temp_original = (bs$temp * 47) - 8,
         atemp_original = (bs$atemp * 66) - 16,
         hum_original = hum * 100,
         windspeed_original = windspeed * 67)

# verify changes
bs %>%
  select(temp_original,atemp_original,
         hum_original, windspeed_original) %>%
  head(.,3)
```


```{r verify cnt total}
# verify cnt = casual + registered
# 0 if correct, 1 if incorrect
cnt_ver <- ifelse(bs$cnt == (bs$registered + bs$casual), 0, 1)
# verify that sum of cnt_ver is 0
sum(cnt_ver)
```

Some column names are not intuitive, so it is better that they are changed.
```{r rename columns}
bs <- bs %>%
  rename(replace = c('dteday' = 'date',
       'weathersit' = 'weather',
       'cnt' = 'total_bikes'))
```

```{r}
# check dataframe
head(bs, 3)
```

# 3. Problem definition
The big question we want to answer using this dataset is how can we predict the number of bikes rented at a certain date and hour, together with other variables such as weather conditions or the type of user.

In order to build the prediction model, we would need to explore and examine not only individual variables, but also the relationship among multiple variables. The result of the analysis will allow us to choose the most appropriate variables to build a model that would help us predict the number of bikes that will be rented.

Thus the rest of this report will follow the following structure:

  4: variable analysis
  5. statistical tests using variables
  6. regression analysis


# 4. Variable analysis
In this section, we will look at the important variables of this datset, and examine the relationships among multiple variables.

```{r variables of dataset}
names(bs)
```


## 4-1. Who are the users?
There are two type of users of Capital Bikeshare: registered users of the company who have membership, and casual users who borrow bikes for one time purposes.

```{r types of users}
sum(bs$casual) / sum(bs$total_bikes)
sum(bs$registered) / sum(bs$total_bikes)
```
Around 81.2 % of the total bikes were borrowed by registered users, and the rest by casual users. There are a lot more registered users than there are casual users, which is true because not all casual users will be using this company's bike only (other companies, own bike).

```{r types of users - histogram}
# distribution of total bikes per day
summary(bs$total_bikes)

grid.arrange(
  ggplot(bs, aes(casual)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Bikes borrowed per hour by casual users") +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(registered)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Bikes borrowed per hour by registered users")+
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(x = total_bikes)) +
    geom_histogram(color = I('gray')) +
    geom_vline(xintercept = mean(bs$total_bikes), color = I('red'), linetype = 2) +
    geom_vline(xintercept = median(bs$total_bikes), color = I('blue'), linetype = 2) +
    ggtitle("Total number of bikes borrowed per hour")+
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ncol = 2
)
```

or both types of users, the number is skewed to the left, which makes sense because it would be really rare to have a lot of users (say 700) using the service at the same time. Because there are a lot more registered users, the distribution of total bikes resembles that of a registered user count more than it does the casual user count distribution. The mean number of rides per day 189.5 (red), but the median is 142 (blue), meaning that the number of ridership is skewed to the left, as seen in the histogram.

Because there are so many registered users compared to the casual users, the usage data related to registered would have a much bigger impact on the total usage data. Therefore, in order to ensure that the casual users' unique usage is not hidden by the mass registered users', we will need to look at the data separately between the two types of users for subsequent analysis.

## 4.2. When do people use bikes?
This dataset provides not only date data, but also hourly data, meaning that we can look at the bike usage pattern at different times of the day.

```{r total borrowed at each hour}
ggplot(bs, aes(x = hr, y = total_bikes)) +
  geom_col() +
  ggtitle("Total number of bikes rented by hour") +
  scale_x_continuous(breaks = seq(0,23,1)) +
  scale_y_continuous(breaks = seq(0,350000,50000)) +
  theme(plot.title = element_text(size = 15, face = "bold"))
```

The number peaks at 9am and 5pm~6pm, which is the usual rush hour commute time. Let's look into this further and see if the usage pattern is same for both the registered and the casual users.

```{r total borrowed at each hour by user types}
# hourly bike rent count for casual vs registered users
grid.arrange(
  ggplot(bs, aes(x = hr, y = casual)) +
    geom_col() +
    ggtitle("Bikes rented at each hour by casual users") +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(x = hr, y = registered)) +
    geom_col() +
    ggtitle("Bikes rented at each hour by registered users") +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ncol = 2
)
```

It is clear that there is a difference in the distribution of number of bikes rented per hour between the casual and regisetered users. This may be because the two types of users have different purposes when borrwing the bike. The peaks seen from total number of rentals are not visible from the casual users' distribution anymore. In fact, there, seems to be a single wide peak for the casual users, which is around the after noon from 12 to 5. This is clearly a working hour during a weekday, which raises another question: do casual and registered users ride primarily on different types of days (i.e., working days vs non working days)? 

First, let's look at how many workingdays and non-working days (holidays and weekends) there are.

```{r workingdays vs nonworking days bar graph}
ggplot(bs, aes(x = factor(workingday))) +
  geom_bar(aes(y = ..count../sum(..count..) * 100)) +
  scale_y_continuous(breaks = seq(0,80,10)) +
  ggtitle("Number of workingdays and nonworking days") +
  theme(plot.title = element_text(size = 15, face = "bold")) +
  ylab("percentage (%)")
```

A little over 30% of the days are either weekends or holidays.

Now, let's look at how the number of bikes borrowed at each hour by the registered and casual users change during workingdays and non-working days.

```{r hourly bike - working vs nonworking days}
grid.arrange(
  ggplot(bs, aes(x = hr, y = registered)) + 
    geom_col() +
    scale_x_continuous(breaks = seq(0,23,1)) +
    facet_wrap(~factor(workingday)),
  
  ggplot(bs, aes(x = hr, y = casual)) + 
  geom_col() +
  scale_x_continuous(breaks = seq(0,23,1)) +
  facet_wrap(~factor(workingday)),
  
  top = "Number of bikes borrowed at each hour \nby user types for working days and non-working day"
)
```

We can see that while the workingday vs non-working day factor has a huge impact on the hourly number of bikes borrowed, the impact is less evident for the casual users. In fact, judging from the graph, it seems that the number of bikes borrowed are similar for both working and non-working days when it comes to casual users. 

```{r number of bikes borrowed by workingday vs non-working day}
grid.arrange(
  ggplot(bs, aes(x = factor(workingday), y = casual)) +
    geom_col() +
    ggtitle("Bikes borrowed by casual users by type of day") +
    xlab("Casual users") +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(x = factor(workingday), y = registered)) + 
    geom_col() +
    ggtitle("Bikes borrowed by registered users by type of day") +
    xlab("Registered users") +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ncol = 2
)
```

While the number of bikes rented for working days and non-working days indeed is similar for casual users (slightly more during non-working days), it is clear that for the registered users, they predominantly use the bikes on working days. This would explain why the two peaks in the hourly distribution were at the commute time: many registered users are using the bikes as a transporation method for commuting to and from work (or school). Since the usage pattern for the two groups are clearly different, this may mean that we might need a separate model to predict the number of bikes rented for each types of users.



Since a significant number of registered users use the bike as a transportation method, we would expect that the number of rideships will not vary too much by season. On the same note, because half of the casual users ride during non-work days (i.e., for leisure), there should be some difference in rideships depending on the season, as weather might be a more important consideration.

```{r number of bikes by workingday, season, and type of user}

grid.arrange(
  ggplot(bs, aes(x = factor(workingday), y = casual)) +
    geom_col() +
    ggtitle("Bikes rented by workingday \nand holiday for casual users") +
    xlab("Casual users") +
    facet_wrap(~season) +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(x = factor(workingday), y = registered)) + 
    geom_col() +
    ggtitle("Bikes rented by workingday \nand holiday for registered users") +
    xlab("Registered users") +
    facet_wrap(~season) +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ncol = 2
)
```

While the expectation seems to hold true for the registered group (i.e., rideships don't vary too much by season) except in the winter when the number of rideships decrease in general, there seems to be no big differene in rideships for working days and non-working days for the casual groups too for each season. A further hypothesis test should be conducted to see if the working day and season categories are independent from one another for the casual users.


## 4.3. How does the weather affect rideship?
Riding a bike is different from many other modes of transporation, as the rider is usually fully exposed to the environment during the ride. As such is the case, weather conditions, including the actual weather situation, temperature, humidity, and wind, are all important factors that may influence the number of bikes used (or borrwed in this case).

What weather was the most common in the dataset?

```{r weather type count}
ggplot(bs, aes(x = weather))+
  geom_bar() +
  ggtitle("Occurances of each weather type") +
  theme(plot.title = element_text(size = 15, face = "bold"))
```

While milder weathers are the most common, the harshest weathers including a snow storm or thundertorm are far less common in the dataset. Are these weather patterns evenly seen in all seasons?

```{r weather conditions by each season}
ggplot(bs, aes(x = season,
               y = ..prop.., group = 1)) +
  stat_count(show.legend = F) +
  facet_wrap(~factor(weather)) +
  geom_text(stat = 'count',
            aes(label = sprintf("%0.2f",
                                round(..prop.., digits = 2))),
            vjust = 0,
            size = 2) +
  
  ggtitle("Proportion of each season by weather type") +
  theme(plot.title = element_text(size = 15, face = "bold"))
```

We can see that the harshest weather only occur in Winter, and though the number of harsh weather (4) is small, this may have an impact on the average number of daily ridership for Winter.

Would each of these weather have an impact on the number of bikes borrowed by each user?

```{r number of bikes by weather and type of users - boxplot}
grid.arrange(
  ggplot(bs, aes(y = casual, x = factor(weather))) +
    geom_boxplot(),

  ggplot(bs, aes(y = registered, x = factor(weather))) +
    geom_boxplot(),

  ncol = 2,
  top = "Distribution of bikes borrowed by each weather type and user"
)
```

Number of total bike rented decreases as the weather gets harsher, which is predictable. It is however interesting to see that there are outliers in all weather conditions (except the harshest 4), meaning that there are significant number of people who use bikes regardless of some weather changes. Would we see similar patterns for each season?

```{r number of bikes by season and type of users - boxplot}
grid.arrange(
  ggplot(bs, aes(y = casual, x = season)) +
    geom_boxplot(),

  ggplot(bs, aes(y = registered, x = season)) +
    geom_boxplot(),

  ncol = 2,
  top = "Distribution of bikes borrowed by season and user"
)
```

Unsurprisingly, average daily rideship decreases during winter, possibly due to factor such as weather condition or temperature. It is interesting to see that there are outliers in the top in all seasons, meaning that there are some people who ride bikes regardless of the season. These people may be riding bikes not for leisure, but for transportation means. Furthermore, this observation further confirms that registered users are less impacted by weather factors such as season and weather situation, as they use bikes for transporation means.


While seasons and weather situations are important to look at, those are aggregated data of different days with different temperatures, humidity, and windspeed. Let's look at the specific weather conditions. 

There are two types of temperature given in this datset - the normal air temperature, and the apparent temperature, which is the temperature actually perceived by humans, accounting for other weather  conditions such as humidity and windspeed. A natural question therefore would be whether or not air temperature and apparent temperature similar. 

```{r temp vs atemp distribution}
grid.arrange(
  ggplot(bs, aes(temp_original)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Air temperature distribution") +
    scale_y_continuous(breaks = seq(0,2100,300), limits = c(0,2100)) +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ggplot(bs, aes(atemp_original)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Apparent temperature dyistribution") +
    scale_y_continuous(breaks = seq(0,2100,300), limits = c(0,2100)) +
    theme(plot.title = element_text(size = 10, face = "bold")),
  
  ncol = 2
)
```

The overall shape seems to be similar except for the peark around 27 degrees in atemp_original. Since apparent temperature is affected by not only temperature but also other weather conditions such as humidity and wind speed, this may explain why the distribution is not exactly equal to one another.

```{r summary and distribution of temp vs atemp}
summary(bs$atemp_original)
summary(bs$temp_original)

ggplot(bs) +
  geom_boxplot(aes(x = factor('temperature'),y = temp_original)) +
  geom_boxplot(aes(x = factor('app_temperature'),y = atemp_original)) +
  scale_y_continuous(breaks = seq(-20,50,10)) +
  ggtitle("Distribution of temperature and apparent temperature") +
  theme(plot.title = element_text(size = 15, face = "bold"))
```

It also seems that the apparent temperature is more sparsely distributed than temperature, as shown by the larger IQR box height and the min and max whisker. The median, however, is more or less smiliar around 15 ~ 16 degrees. Let's look at this further by each season, where temperature should differ significantly.

```{r temp vs atemp distribution b y each season}
# data needs to be gathered to achieve this

bs_melt <- bs %>%
  select(temp_original, atemp_original,season) %>%
  gather(key = temp_type, value = temp_value, -season)

head(bs_melt)

# boxplot comparing temperature for each season
ggplot(bs_melt, aes(x = season, y = temp_value, fill = temp_type)) +
  geom_boxplot() +
  ggtitle("Distribution of temperature and apparente temperature by season") + 
  theme(plot.title = element_text(size = 13, face = "bold"))
```

It is interesting to see that the apparent temperature is often more extreme than the actual air temperature. For example, the apparent temperature is higher than the actual air temperature during summer, while it is on average lower than the actual temperature during winter. 

The important question is then, does the ridership differ by each temperature for each users?

```{r bikes borrowed by temp, atemp and user}
# does the ridership differ for each type of user by temperature?
grid.arrange(
    ggplot(bs, aes(x = temp_original)) +
    geom_point(aes(y = casual), 
               color = I('red'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = temp_original)) +
    geom_point(aes(y = registered), 
               color = I('blue'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = atemp_original)) +
    geom_point(aes(y = casual), 
               color = I('red'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = atemp_original)) +
    geom_point(aes(y = registered), 
               color = I('blue'), 
               alpha = 0.3, position = 'jitter'),
  
  ncol = 2,
  
  top = "Bikes borrowed by each type of user and temperature vs apparent temperature"
)
```

Interestingly, there doesn't seem to be a big pattern, other than the fact that there seems to be some divison in different points of apparent temperature for both the casual (around 20 degrees) and the registered (around 10 and 20 degrees) users.

Let's look at humidity and windspeed next. How are humidity and windspeed distributed in the dataset?

```{r humidity windspeed distribution}
summary(bs$hum_original)
summary(bs$windspeed_original)

grid.arrange(
  ggplot(bs, aes(hum_original)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Humidity distribution") +
    theme(plot.title = element_text(size = 12, face = "bold")),
  
  ggplot(bs, aes(windspeed_original)) +
    geom_histogram(color = I('gray')) +
    ggtitle("Windspeed distribution") +
    theme(plot.title = element_text(size = 12, face = "bold")),
  
  ncol = 2
)
```

Both the humidity and windspeed seemed to be somewhat skewed, especially the windspeed. According to the Beaufort wind force scale classication (https://en.wikipedia.org/wiki/Beaufort_scale), wind speed between 20 to 28 is considered a moderate breeze, between 29 to 38 fresh breeze, between 39 to 49 strong breeze, and between 50 to 61 high wind (moderate gale). This means that most of the days, the windspeed was lower than a moderate breeze, and that there were few days with very strong winds that may affect bike ridership.

Does the windspeed and humidity differ by each season as do the temperature?

```{r windspeed by season}
# windspeed for each season
ggplot(bs, aes(x = season, y = windspeed_original)) +
  geom_boxplot() +
  ggtitle("Windspeed by season") +
  theme(plot.title = element_text(size = 15, face = "bold"))
```

```{r humidity by season}
# humidity for each season
ggplot(bs, aes(x = season, y = hum_original)) + 
  geom_boxplot() +
  ggtitle("Humidity by season") +
  theme(plot.title = element_text(size = 15, face = "bold"))
```


```{r}
# humidity and windspeed?
grid.arrange(
    ggplot(bs, aes(x = hum_original)) +
    geom_point(aes(y = casual), 
               color = I('red'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = hum_original)) +
    geom_point(aes(y = registered), 
               color = I('blue'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = windspeed_original)) +
    geom_point(aes(y = casual), 
               color = I('red'), 
               alpha = 0.3, position = 'jitter'),
  
  ggplot(bs, aes(x = windspeed_original)) +
    geom_point(aes(y = registered), 
               color = I('blue'), 
               alpha = 0.3, position = 'jitter'),
  
  ncol = 2
)
```


## conclusion: Variable analysis conclusion
While weather conditions all have impact on ridership in general, it is also true that the level of impact differs for each type of user. In the statistical test, it would be interesting to further this observation and see if the differences are significant.

# 5. Statistical tests

## 5.1 Is there a significant difference between the actual air temperature and the apparent temperature perceived by humans in terms of number of rideships?
```{r}
# two-tailed, 2 independent variables t-test, 95% confidence level
t.test(bs$temp_original, bs$atemp_original, alternative = 'two.sided', paired = T, mu = 0)
```
The p-value is 0.043, which means that there is enough evidence to prove that the air temperature and the temperature perceived by humans differ significantly. 

##5.2 Does season and workingday have an effect on rideships for each type of users? If so, do the two effects interact?
```{r}
summary(aov(data = bs, casual ~ season + factor(workingday) + season:factor(workingday)))
summary(aov(data = bs, registered ~ season + factor(workingday) + season:factor(workingday)))
```
This confirms the hypothesis formed during the multivariate analysis. While season and workingday both have an impact on the number of bikes rented for both types of users, the interaction effect between the two variables don't exist for registered users, while it does for the causal users. Again, this is because registered users, who mostly use the bikes for commute purposes, are less impacted by seasonal factors than are casual users, half of whom ride bikes for leisure. We can also confirm the interaction effect visually:

```{r}
interaction.plot(bs$season, bs$workingday, bs$casual)
interaction.plot(bs$season, bs$workingday, bs$registered)
```
From the interaction plot, it is clear that season has an impact on ridership for casual users. More specifically, the number of bikes rented for non-working days drop more drastically in Winter than it does for working days. On the contrast, the interaction plot for the registered users show that the patterns of total number of bikes are similar (if not the same), regardless of the season.


## 5.3 Does the time of the day has a significant impact on ridership?
H0: No difference in ridership with time of the day
H1: There is some difference
For this purpose we create a categorical variable from hr - hr_cat:
1. Late Night
2. Early Morning
3. Afternoon
4. Evening/Night

```{r}
bs= bs %>%
  mutate(hr_cat=ifelse(hr>=0 & hr<=5,"1",
                       ifelse(hr>=6 & hr <=11,2,
                              ifelse(hr>=12 & hr<=17,3,4))))
a1=aov(data = bs, total_bikes ~ hr_cat)
summary(a1)
```

Based on the above result (p-value<0.05) with 95% confidence, we reject H0 and conclude that there is some difference.

Now lets find out when the ridership is the highest.
```{r}
a2=TukeyHSD(a1)
print(a2)
plot(a2)
```

Looking at the above chart we can rank the demand.
$ Ridership in Afternoon> Evening/ Night > Early Morning> Late Night$
1. Ridership is the highest in Afternoon
2. Ridership is the lowest in Late Night 


## 5.4 Does ridership depend on type of day?
Lets define three types of days:
1. Holiday
2. Working day
3. Weekend

H0: No difference in ridership with type of day
H1: There is some difference

```{r}
bs= bs %>%
  mutate(typeofday = ifelse(holiday==0 & workingday==0,"Weekend",
                            ifelse(holiday ==1, "Holiday", "Working Day" )))
b1 = aov(data=bs, total_bikes ~ typeofday )
summary(b1)
```

Since the p-value is less than .05 we can reject H0 and conclude that the rideship is different during different types of days.

Now again lets find out on which type of days the ridership is higher.

```{r}
b2= TukeyHSD(b1)
print(b2)
plot(b2)
```
Conclusion: Taking a look at the above table and the plot we can conclude that - 
$Ridership on Working Day > Ridership on Weekend > Ridership on Holiday$
 
## 5.5 Does rideship depend on the weather conditions?
Lets see if the type f weather has an impact on ridership.

H0: Type of weather has no impact on riderwhip
H1: There is some impact

```{r}
c1 = aov(data=bs, total_bikes ~ factor(weather, levels = c(1,2,3)))
summary (c1)
```

Since the p-value is <0.05 we cab conclude that the ridership depends on the weather. 
Now lets find out how the weather affects the ridership.

```{r}
c2 = TukeyHSD(c1)
print(c2)
plot(c2)
```
Looking at the above data we can conclude that:
$Ridership in Clear weather > Mist > Light Snow/rain $ 


# 6. Regression analysis
We can use the variables we have explored to predict how many bikes will be borrowed in a given hour and day?

## 6.1 Checking linear relation between independent and dependent variables

# we can run a correlation matrix for some of the numerical variables, to check beforehand if any of them are highly correlated. 
```{r correlation matrix}
aux= bs %>%
  dplyr::select(-date,-season, -yr, -holiday, -weekday, -temp, -atemp, -hum, -windspeed, -hr_cat, -typeofday)
head(aux)
caux=cor(aux)
corrplot(caux, method="number", order="FPC", type="lower",tl.col="black", tl.cex=0.7, number.cex=0.7, cl.cex=0.7)
```
Based on the matrix only, we don't see huge problems of corrleation between potential independent variables.


## 6.2 Selecting variables for the model

We can use the step-wise method to choose the most significant variables for our linear regression model.

```{r stepwise method}
# filter out unwanted 
bs2 = bs%>%
  filter(yr == 1) %>% # to reduce the impact of time correlation from different years data, we only select one of the two years.
  dplyr::select(-registered,-date,-yr,-mnth, -weekday, -hr, -temp, -atemp, -hum, -windspeed, -typeofday, -temp_original)

null= lm(data=bs2, total_bikes ~ 1)  # empty model
full = lm(data=bs2, total_bikes ~ .) # full model

step = stepAIC(null, scope=list(lower=null, upper=full), direction = "forward")

step$anova
summary(step)

```


```{r define lms}
# define linear models from the best set of variables
lm1 <- lm(data = bs2, total_bikes ~ casual + hr_cat + workingday + season + weather + 
    atemp_original + hum_original + windspeed_original)
lm2 <- lm(data = bs2, total_bikes ~ casual + hr_cat + workingday + season + weather + 
    atemp_original + hum_original)
lm3 <- lm(data = bs2, total_bikes ~ casual + hr_cat + workingday + season + weather)
lm4 <- lm(data = bs2, total_bikes ~ casual + hr_cat + workingday + season)

```


```{r summary of lms}
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
```

We can now use these models to test the assumptions of linear regression. We can run the gvlma function to quickly test the assumptions of each of the linear regression models.

```{r}
# run assumptions on the models
gvlma(lm1)
gvlma(lm2)
gvlma(lm3)
gvlma(lm4)
```

We can see that for all of the models the homoscedasticity assumption is not held. We can choose one of the linear model manually verify important assumptions of linear regression.

```{r manual lr assumptions}

# check homoscedasticity
lm1_df <- data.frame(x = rstandard(lm1))

ggplot(data.frame(x = predict(lm1), y = rstandard(lm1))) +
  geom_point(aes(x, y ))

# check normality of residuals (histogram)
grid.arrange(
  ggplot(lm1_df, aes(x = x)) +
    geom_histogram(color = I('white'), aes(y = ..density..)) +
    stat_function(fun = dnorm, args = list(mean = mean(lm1_df$x),
                                           sd = sd(lm1_df$x)),
                  color = I('red'),
                  linetype = 2),
  
  ggplot(lm1_df, aes(sample = scale(x))) +
    stat_qq() +
    geom_abline(slope = 1, intercept = 0, color = I('red'), linetype = 2),
  
  ncol = 2
)

# check collinearlity of variables
vif(lm1)

# check for outliers
outlierTest(lm1)

# check for influential points that may have an impact on our analysis
plot(cooks.distance(lm1))

# autocorrelation of errors
durbinWatsonTest(lm1)

```

We can see that while there are no significant influencers that can have an effect on the regression model (influencer test), and the variables do not show signs of collinearity (vif test), the residuals don't follow a perfect normal distribution, and the residuals do not hold the assumption of homoscedasity. We can also further check whether our residuals are really heteroscedastic by using the Goldfeld-Quandt test. The null hypothesis is that the residuas are homoscedastic, and the alternative is that they are not. Getting a gq statstic that is more than 0.05 (or the alpha level) will indicate that the model truly has a heteroscedasticity problem.

```{r gqtest}
# test for hereostedcity
gqtest(data = bs2, total_bikes ~ casual + hr_cat + workingday + season + weather + 
    atemp_original + hum_original + windspeed_original)
```
Since the GQ value is 1.41, which is higher than alpha value 0.05, we can reject the null and say that there isa true heteroscedasticty roblem with our model.


### Dual model for each type of users

We were looking for ways to improve our initial model, and based on our variable analysis and statstical tests, we decided that it may yield better models with stronger predictive powers when we divide the prediction model for each type of users, casual and registered. This was because there were some differences in bike usage pattern between the two types of users, and thus separating them would make more sense.

```{r lm for registered users}
# model for registered users
lmr <- lm(data = bs, registered ~ season + factor(hr) + factor(workingday) + factor(weather))
summary(lmr)
```

```{r checking assumptions for lmr}
# check homoscedasticity
lmr_df <- data.frame(x = rstandard(lmr))

ggplot(data.frame(x = predict(lmr), y = rstandard(lmr))) +
  geom_point(aes(x, y ))

# check normality of residuals (histogram)
grid.arrange(
  ggplot(lmr_df, aes(x = x)) +
    geom_histogram(color = I('white'), aes(y = ..density..)) +
    stat_function(fun = dnorm, args = list(mean = mean(lmr_df$x),
                                           sd = sd(lmr_df$x)),
                  color = I('red'),
                  linetype = 2),
  
  ggplot(lmr_df, aes(sample = scale(x))) +
    stat_qq() +
    geom_abline(slope = 1, intercept = 0, color = I('red'), linetype = 2),
  
  ncol = 2
)

# check collinearlity of variables
vif(lmr)

# check for outliers
outlierTest(lmr)

# check for influential points that may have an impact on our analysis
plot(cooks.distance(lmr))

# autocorrelation of errors
durbinWatsonTest(lmr)
```


```{r lm for casual users}
# model for casual users
lmc <- lm(data =bs, casual ~ factor(hr) + factor(weather) + season + windspeed_original + factor(workingday))
summary(lmc)

```


```{r checking assumptions for lmc}
# check homoscedasticity
lmc_df <- data.frame(x = rstandard(lmc))

ggplot(data.frame(x = predict(lmc), y = rstandard(lmc))) +
  geom_point(aes(x, y ))


# check normality of residuals (histogram)
grid.arrange(
  ggplot(lmc_df, aes(x = x)) +
    geom_histogram(color = I('white'), aes(y = ..density..)) +
    stat_function(fun = dnorm, args = list(mean = mean(lmc_df$x),
                                           sd = sd(lmc_df$x)),
                  color = I('red'),
                  linetype = 2),
  
  ggplot(lmc_df, aes(sample = scale(x))) +
    stat_qq() +
    geom_abline(slope = 1, intercept = 0, color = I('red'), linetype = 2),
  
  ncol = 2
)

# check collinearlity of variables
vif(lmc)

# check for outliers
outlierTest(lmc)

# check for influential points that may have an impact on our analysis
plot(cooks.distance(lmc))

# autocorrelation of errors
durbinWatsonTest(lmc)
```

The R2 values are around 0.5 for both models, which means that about half of the information of the predictions are explained by other things other than the variables. Furthermore, the residuals, are not homoscedastic, meaning that somehow the residuals are worse for certain types of observations. Thus, even when separated, the models don't explain the number of bikes rented completely.


# 7. Regression model validation
Now let's validate our 2 user models by using k-folds cross validation.

# create training set and testing results for casual model
```{r}
trainingFold1 = createDataPartition(bs$casual, p = 0.8)
training1 = bs[trainingFold1$Resample1, ]
testing1  = bs[-trainingFold1$Resample1, ]


trainMethod = trainControl(method="cv", number=5, returnData =TRUE, returnResamp = "all")
model_casual = train(data=training1, casual ~ factor(hr) + factor(weather) + season + windspeed_original + factor(workingday), method = 'lm')
summary(model_casual)
```

# create training set and testing results for registered model
```{r}
trainingFold2 = createDataPartition(bs$registered, p = 0.8)
training2 = bs[trainingFold2$Resample1, ]
testing2  = bs[-trainingFold2$Resample1, ]

trainMethod = trainControl(method="cv", number=5, returnData =TRUE, returnResamp = "all")
model_registered = train(data=training2, registered ~ season + factor(hr) + factor(workingday) + factor(weather), method = 'lm')
summary(model_registered)
```

Again, the r2 for both models are still at 51~ 59%, meaning that we may need more variables that may explain the total number of bikes better.


# 8. Conclusion
While we have had some interesting insights for our dataset, we weren't able to build a sastisfying model despite our efforts:

  * Different models: we have created various models playing with different variables to predict the total number of bikes borrowed. We also created two separate models for different types of users, as the users have different usage patterns.
  * Numerous tests: we have tested the models against numerous tests to see if the assumptions of linear regression held and what actions we could further do to improve the regression model.
  * Playing with variables: we tried to create new variables (hr_cat) to better explain the model. We tried to remove errors coming from autocorrelated errors due to timeseries data by limiting the dataset to 1 year.

We believe that there are various ways in which this prediction model can be improved:
  * Try other predictive algorithms: this dataset may not be appropriate for linear regression, and thus that may be the reason why the predictive results were not satisfying.
  * More data on customers: each of the row in this dataset is actually an aggregate of customers by each hour. This means that there are lack of customer related data, especially on an individual level. It would be nice to have more information on individual customers that may help improve the prediction power (e.g. age, gender, nationality etc.), as these personal variables may also have an impact on the usage pattern. Currently, the only thing we know about the customers themselves are whether they are registered or casual customers, and to assume all of them would borrow bieks for similar reasons and patterns would be a big mistake.
